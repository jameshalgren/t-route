{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic imports and path management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "root = pathlib.Path(\"../../\").resolve()\n",
    "sys.path.append(str(root.joinpath(\"src\", \"python_framework_v02\")))\n",
    "sys.path.append(str(root.joinpath(\"src\", \"python_framework_v01\")))\n",
    "sys.path.append(str(root.joinpath(\"src\", \"python_routing_v01\")))\n",
    "sys.path.append(\".\")\n",
    "import nhd_io as nio\n",
    "import compute_nhd_routing_SingleSeg as tr\n",
    "import nhd_network_utilities_v01 as nnu\n",
    "import nhd_reach_utilities as nru\n",
    "\n",
    "custom_input_folder = root.joinpath(\"test\", \"input\", \"yaml\")\n",
    "custom_input_file = \"florence_933020089_dt60.yaml\"\n",
    "run_pocono2_test = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the primary data input from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supernetwork_parameters = None\n",
    "waterbody_parameters = None\n",
    "if custom_input_file:\n",
    "    (\n",
    "        supernetwork_parameters,\n",
    "        waterbody_parameters,\n",
    "        forcing_parameters,\n",
    "        restart_parameters,\n",
    "        output_parameters,\n",
    "        run_parameters,\n",
    "    ) = nio.read_custom_input(custom_input_folder.joinpath(custom_input_file))\n",
    "\n",
    "    break_network_at_waterbodies = run_parameters.get(\n",
    "        \"break_network_at_waterbodies\", None\n",
    "    )\n",
    "\n",
    "    dt = run_parameters.get(\"dt\", None)\n",
    "    nts = run_parameters.get(\"nts\", None)\n",
    "    qts_subdivisions = run_parameters.get(\"qts_subdivisions\", None)\n",
    "    debuglevel = -1 * int(run_parameters.get(\"debuglevel\", 0))\n",
    "    verbose = run_parameters.get(\"verbose\", None)\n",
    "    showtiming = run_parameters.get(\"showtiming\", None)\n",
    "    percentage_complete = run_parameters.get(\"percentage_complete\", None)\n",
    "    do_network_analysis_only = run_parameters.get(\"do_network_analysis_only\", None)\n",
    "    assume_short_ts = run_parameters.get(\"assume_short_ts\", None)\n",
    "    parallel_compute = run_parameters.get(\"parallel_compute\", None)\n",
    "    cpu_pool = run_parameters.get(\"cpu_pool\", None)\n",
    "    sort_networks = run_parameters.get(\"sort_networks\", None)\n",
    "\n",
    "    csv_output = output_parameters.get(\"csv_output\", None)\n",
    "    nc_output_folder = output_parameters.get(\"nc_output_folder\", None)\n",
    "\n",
    "    qlat_const = forcing_parameters.get(\"qlat_const\", None)\n",
    "    qlat_input_file = forcing_parameters.get(\"qlat_input_file\", None)\n",
    "    qlat_input_folder = forcing_parameters.get(\"qlat_input_folder\", None)\n",
    "    qlat_file_pattern_filter = forcing_parameters.get(\"qlat_file_pattern_filter\", None)\n",
    "    qlat_file_index_col = forcing_parameters.get(\"qlat_file_index_col\", None)\n",
    "    qlat_file_value_col = forcing_parameters.get(\"qlat_file_value_col\", None)\n",
    "\n",
    "    wrf_hydro_channel_restart_file = restart_parameters.get(\n",
    "        \"wrf_hydro_channel_restart_file\", None\n",
    "    )\n",
    "    wrf_hydro_channel_ID_crosswalk_file = restart_parameters.get(\n",
    "        \"wrf_hydro_channel_ID_crosswalk_file\", None\n",
    "    )\n",
    "    wrf_hydro_channel_ID_crosswalk_file_field_name = restart_parameters.get(\n",
    "        \"wrf_hydro_channel_ID_crosswalk_file_field_name\", None\n",
    "    )\n",
    "    wrf_hydro_channel_restart_upstream_flow_field_name = restart_parameters.get(\n",
    "        \"wrf_hydro_channel_restart_upstream_flow_field_name\", None\n",
    "    )\n",
    "    wrf_hydro_channel_restart_downstream_flow_field_name = restart_parameters.get(\n",
    "        \"wrf_hydro_channel_restart_downstream_flow_field_name\", None\n",
    "    )\n",
    "    wrf_hydro_channel_restart_depth_flow_field_name = restart_parameters.get(\n",
    "        \"wrf_hydro_channel_restart_depth_flow_field_name\", None\n",
    "    )\n",
    "\n",
    "    wrf_hydro_waterbody_restart_file = restart_parameters.get(\n",
    "        \"wrf_hydro_waterbody_restart_file\", None\n",
    "    )\n",
    "    wrf_hydro_waterbody_ID_crosswalk_file = restart_parameters.get(\n",
    "        \"wrf_hydro_waterbody_ID_crosswalk_file\", None\n",
    "    )\n",
    "    wrf_hydro_waterbody_ID_crosswalk_file_field_name = restart_parameters.get(\n",
    "        \"wrf_hydro_waterbody_ID_crosswalk_file_field_name\", None\n",
    "    )\n",
    "    wrf_hydro_waterbody_crosswalk_filter_file = restart_parameters.get(\n",
    "        \"wrf_hydro_waterbody_crosswalk_filter_file\", None\n",
    "    )\n",
    "    wrf_hydro_waterbody_crosswalk_filter_file_field_name = restart_parameters.get(\n",
    "        \"wrf_hydro_waterbody_crosswalk_filter_file_field_name\", None\n",
    "    )\n",
    "\n",
    "# Any specific commandline arguments will override the file\n",
    "# TODO: There are probably some pathological collisions that could\n",
    "# arise from this ordering ... check these out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_pocono2_test:\n",
    "    if verbose:\n",
    "        print(\"running test case for Pocono_TEST2 domain\")\n",
    "    # Overwrite the following test defaults\n",
    "    supernetwork = \"Pocono_TEST2\"\n",
    "    break_network_at_waterbodies = False\n",
    "    qts_subdivisions = 1  # change qts_subdivisions = 1 as  default\n",
    "    dt = 300 / qts_subdivisions\n",
    "    nts = 144 * qts_subdivisions\n",
    "    csv_output = {\"csv_output_folder\": os.path.join(root, \"test\", \"output\", \"text\")}\n",
    "    nc_output_folder = os.path.join(root, \"test\", \"output\", \"text\")\n",
    "    # test 1. Take lateral flow from re-formatted wrf-hydro output from Pocono Basin simulation\n",
    "    qlat_input_file = os.path.join(\n",
    "        root, r\"test/input/geo/PoconoSampleData2/Pocono_ql_testsamp1_nwm_mc.csv\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if showtiming:\n",
    "    program_start_time = time.time()\n",
    "if verbose:\n",
    "    print(f\"begin program t-route ...\")\n",
    "\n",
    "# STEP 1: Read the supernetwork dataset and build the connections graph\n",
    "if verbose:\n",
    "    print(\"creating supernetwork connections set\")\n",
    "if showtiming:\n",
    "    start_time = time.time()\n",
    "\n",
    "if supernetwork_parameters:\n",
    "    supernetwork_values = nnu.get_nhd_connections(\n",
    "        supernetwork_parameters=supernetwork_parameters,\n",
    "        verbose=False,\n",
    "        debuglevel=debuglevel,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    test_folder = os.path.join(root, r\"test\")\n",
    "    geo_input_folder = os.path.join(test_folder, r\"input\", r\"geo\")\n",
    "    supernetwork_parameters, supernetwork_values = nnu.set_networks(\n",
    "        supernetwork=supernetwork,\n",
    "        geo_input_folder=geo_input_folder,\n",
    "        verbose=False,\n",
    "        debuglevel=debuglevel,\n",
    "    )\n",
    "    waterbody_parameters = nnu.set_waterbody_parameters(\n",
    "        supernetwork=supernetwork,\n",
    "        geo_input_folder=geo_input_folder,\n",
    "        verbose=False,\n",
    "        debuglevel=debuglevel,\n",
    "    )\n",
    "\n",
    "if verbose:\n",
    "    print(\"supernetwork connections set complete\")\n",
    "if showtiming:\n",
    "    print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "connections = supernetwork_values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Separate the networks and build the sub-graph of reaches within each network\n",
    "if showtiming:\n",
    "    start_time = time.time()\n",
    "if verbose:\n",
    "    print(\"organizing connections into reaches ...\")\n",
    "networks = nru.compose_networks(\n",
    "    supernetwork_values,\n",
    "    break_network_at_waterbodies=break_network_at_waterbodies,\n",
    "    verbose=False,\n",
    "    debuglevel=debuglevel,\n",
    "    showtiming=showtiming,\n",
    ")\n",
    "\n",
    "if verbose:\n",
    "    print(\"reach organization complete\")\n",
    "if showtiming:\n",
    "    print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Organize Network for Waterbodies\n",
    "if break_network_at_waterbodies:\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"reading waterbody parameter file ...\")\n",
    "\n",
    "    ## STEP 3a: Read waterbody parameter file\n",
    "    waterbodies_values = supernetwork_values[12]\n",
    "    waterbodies_segments = supernetwork_values[13]\n",
    "    connections_tailwaters = supernetwork_values[4]\n",
    "\n",
    "    waterbodies_df = nio.read_waterbody_df(waterbody_parameters, waterbodies_values,)\n",
    "    waterbodies_df = waterbodies_df.sort_index(axis=\"index\").sort_index(axis=\"columns\")\n",
    "\n",
    "    nru.order_networks(connections, networks, connections_tailwaters)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"waterbodies complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "\n",
    "    ## STEP 3b: Order subnetworks above and below reservoirs\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"ordering waterbody subnetworks ...\")\n",
    "\n",
    "    max_network_seqorder = -1\n",
    "    for network in networks:\n",
    "        max_network_seqorder = max(\n",
    "            networks[network][\"network_seqorder\"], max_network_seqorder\n",
    "        )\n",
    "    ordered_networks = {}\n",
    "\n",
    "    for terminal_segment, network in networks.items():\n",
    "        if network[\"network_seqorder\"] not in ordered_networks:\n",
    "            ordered_networks[network[\"network_seqorder\"]] = []\n",
    "        ordered_networks[network[\"network_seqorder\"]].append(\n",
    "            (terminal_segment, network)\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"ordering waterbody subnetworks complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "\n",
    "else:\n",
    "    # If we are not splitting the networks, we can put them all in one order\n",
    "    max_network_seqorder = 0\n",
    "    ordered_networks = {}\n",
    "    ordered_networks[0] = [\n",
    "        (terminal_segment, network) for terminal_segment, network in networks.items()\n",
    "    ]\n",
    "\n",
    "if do_network_analysis_only:\n",
    "    sys.exit()\n",
    "\n",
    "if break_network_at_waterbodies:\n",
    "    ## STEP 3c: Handle Waterbody Initial States\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"setting waterbody initial states ...\")\n",
    "\n",
    "    if wrf_hydro_waterbody_restart_file:\n",
    "\n",
    "        waterbody_initial_states_df = nio.get_reservoir_restart_from_wrf_hydro(\n",
    "            wrf_hydro_waterbody_restart_file,\n",
    "            wrf_hydro_waterbody_ID_crosswalk_file,\n",
    "            wrf_hydro_waterbody_ID_crosswalk_file_field_name,\n",
    "            wrf_hydro_waterbody_crosswalk_filter_file,\n",
    "            wrf_hydro_waterbody_crosswalk_filter_file_field_name,\n",
    "        )\n",
    "    else:\n",
    "        # TODO: Consider adding option to read cold state from route-link file\n",
    "        waterbody_initial_ds_flow_const = 0.0\n",
    "        waterbody_initial_depth_const = 0.0\n",
    "        # Set initial states from cold-state\n",
    "        waterbody_initial_states_df = pd.DataFrame(\n",
    "            0, index=waterbodies_df.index, columns=[\"qd0\", \"h0\",], dtype=\"float32\"\n",
    "        )\n",
    "        # TODO: This assignment could probably by done in the above call\n",
    "        waterbody_initial_states_df[\"qd0\"] = waterbody_initial_ds_flow_const\n",
    "        waterbody_initial_states_df[\"h0\"] = waterbody_initial_depth_const\n",
    "        waterbody_initial_states_df[\"index\"] = range(len(waterbody_initial_states_df))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"waterbody initial states complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Handle Channel Initial States\n",
    "if showtiming:\n",
    "    start_time = time.time()\n",
    "if verbose:\n",
    "    print(\"setting channel initial states ...\")\n",
    "\n",
    "if wrf_hydro_channel_restart_file:\n",
    "\n",
    "    channel_initial_states_df = nio.get_stream_restart_from_wrf_hydro(\n",
    "        wrf_hydro_channel_restart_file,\n",
    "        wrf_hydro_channel_ID_crosswalk_file,\n",
    "        wrf_hydro_channel_ID_crosswalk_file_field_name,\n",
    "        wrf_hydro_channel_restart_upstream_flow_field_name,\n",
    "        wrf_hydro_channel_restart_downstream_flow_field_name,\n",
    "        wrf_hydro_channel_restart_depth_flow_field_name,\n",
    "    )\n",
    "else:\n",
    "    # TODO: Consider adding option to read cold state from route-link file\n",
    "    channel_initial_us_flow_const = 0.0\n",
    "    channel_initial_ds_flow_const = 0.0\n",
    "    channel_initial_depth_const = 0.0\n",
    "    # Set initial states from cold-state\n",
    "    channel_initial_states_df = pd.DataFrame(\n",
    "        0, index=connections.keys(), columns=[\"qu0\", \"qd0\", \"h0\",], dtype=\"float32\"\n",
    "    )\n",
    "    channel_initial_states_df[\"qu0\"] = channel_initial_us_flow_const\n",
    "    channel_initial_states_df[\"qd0\"] = channel_initial_ds_flow_const\n",
    "    channel_initial_states_df[\"h0\"] = channel_initial_depth_const\n",
    "    channel_initial_states_df[\"index\"] = range(len(channel_initial_states_df))\n",
    "\n",
    "if verbose:\n",
    "    print(\"channel initial states complete\")\n",
    "if showtiming:\n",
    "    print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Read (or set) QLateral Inputs\n",
    "if showtiming:\n",
    "    start_time = time.time()\n",
    "if verbose:\n",
    "    print(\"creating qlateral array ...\")\n",
    "\n",
    "# initialize qlateral dict\n",
    "qlateral = {}\n",
    "\n",
    "if qlat_input_folder:\n",
    "    qlat_files = []\n",
    "    for pattern in qlat_file_pattern_filter:\n",
    "        qlat_files.extend(glob.glob(qlat_input_folder + pattern))\n",
    "    qlat_df = nio.get_ql_from_wrf_hydro(\n",
    "        qlat_files=qlat_files,\n",
    "        index_col=qlat_file_index_col,\n",
    "        value_col=qlat_file_value_col,\n",
    "    )\n",
    "\n",
    "elif qlat_input_file:\n",
    "    qlat_df = nio.get_ql_from_csv(qlat_input_file)\n",
    "\n",
    "else:\n",
    "    qlat_df = pd.DataFrame(\n",
    "        qlat_const, index=connections.keys(), columns=range(nts), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "for index, row in qlat_df.iterrows():\n",
    "    qlateral[index] = row.tolist()\n",
    "\n",
    "if verbose:\n",
    "    print(\"qlateral array complete\")\n",
    "if showtiming:\n",
    "    print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Sort the ordered networks\n",
    "if sort_networks:\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"sorting the ordered networks ...\")\n",
    "\n",
    "    for nsq in range(max_network_seqorder, -1, -1):\n",
    "        sort_ordered_network(ordered_networks[nsq], True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"sorting complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pool after we create the static global objects (and collect the garbage)\n",
    "if parallel_compute:\n",
    "    import gc\n",
    "\n",
    "    gc.collect()\n",
    "    pool = multiprocessing.Pool(cpu_pool)\n",
    "\n",
    "flowveldepth_connect = (\n",
    "    {}\n",
    ")  # dict to contain values to transfer from upstream to downstream networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Main Execution Loop across ordered networks\n",
    "if showtiming:\n",
    "    main_start_time = time.time()\n",
    "if verbose:\n",
    "    print(f\"executing routing computation ...\")\n",
    "\n",
    "compute_network_func = tr.compute_network\n",
    "\n",
    "tr.connections_g = connections\n",
    "tr.networks_g = networks\n",
    "tr.qlateral_g = qlateral\n",
    "tr.waterbodies_df_g = waterbodies_df\n",
    "tr.waterbody_initial_states_df_g = waterbody_initial_states_df\n",
    "tr.channel_initial_states_df_g = channel_initial_states_df\n",
    "\n",
    "progress_count = 0\n",
    "percentage_complete = True\n",
    "if percentage_complete:\n",
    "    for nsq in range(max_network_seqorder, -1, -1):\n",
    "        for terminal_segment, network in ordered_networks[nsq]:\n",
    "            progress_count += len(network[\"all_segments\"])\n",
    "    pbar = tqdm(total=(progress_count))\n",
    "\n",
    "for nsq in range(max_network_seqorder, -1, -1):\n",
    "\n",
    "    if parallel_compute:\n",
    "        nslist = []\n",
    "    results = []\n",
    "\n",
    "    current_index_total = 0\n",
    "\n",
    "    for terminal_segment, network in ordered_networks[nsq]:\n",
    "\n",
    "        if percentage_complete:\n",
    "            if current_index_total == 0:\n",
    "                pbar.update(0)\n",
    "\n",
    "        if break_network_at_waterbodies:\n",
    "            waterbody = waterbodies_segments.get(terminal_segment)\n",
    "        else:\n",
    "            waterbody = None\n",
    "        if not parallel_compute:  # serial execution\n",
    "            if showtiming:\n",
    "                start_time = time.time()\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"routing ordered reaches for terminal segment {terminal_segment} ...\"\n",
    "                )\n",
    "\n",
    "            results.append(\n",
    "                compute_network_func(\n",
    "                    flowveldepth_connect=flowveldepth_connect,\n",
    "                    terminal_segment=terminal_segment,\n",
    "                    supernetwork_parameters=supernetwork_parameters,\n",
    "                    waterbody_parameters=waterbody_parameters,\n",
    "                    waterbody=waterbody,\n",
    "                    nts=nts,\n",
    "                    dt=dt,\n",
    "                    qts_subdivisions=qts_subdivisions,\n",
    "                    verbose=verbose,\n",
    "                    debuglevel=debuglevel,\n",
    "                    csv_output=csv_output,\n",
    "                    nc_output_folder=nc_output_folder,\n",
    "                    assume_short_ts=assume_short_ts,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if showtiming:\n",
    "                print(\"... complete in %s seconds.\" % (time.time() - start_time))\n",
    "            if percentage_complete:\n",
    "                pbar.update(len(network[\"all_segments\"]))\n",
    "\n",
    "        else:  # parallel execution\n",
    "            nslist.append(\n",
    "                [\n",
    "                    flowveldepth_connect,\n",
    "                    terminal_segment,\n",
    "                    supernetwork_parameters,  # TODO: This should probably be global...\n",
    "                    waterbody_parameters,\n",
    "                    waterbody,\n",
    "                    nts,\n",
    "                    dt,\n",
    "                    qts_subdivisions,\n",
    "                    verbose,\n",
    "                    debuglevel,\n",
    "                    csv_output,\n",
    "                    nc_output_folder,\n",
    "                    assume_short_ts,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    if parallel_compute:\n",
    "        if verbose:\n",
    "            print(f\"routing ordered reaches for networks of order {nsq} ... \")\n",
    "        if debuglevel <= -2:\n",
    "            print(f\"reaches to be routed include:\")\n",
    "            print(f\"{[network[0] for network in ordered_networks[nsq]]}\")\n",
    "        # with pool:\n",
    "        # with multiprocessing.Pool() as pool:\n",
    "        results = pool.starmap(compute_network_func, nslist)\n",
    "\n",
    "        if showtiming:\n",
    "            print(\"... complete in %s seconds.\" % (time.time() - start_time))\n",
    "        if percentage_complete:\n",
    "            pbar.update(\n",
    "                sum(\n",
    "                    len(network[1][\"all_segments\"]) for network in ordered_networks[nsq]\n",
    "                )\n",
    "            )\n",
    "            # print(f\"{[network[0] for network in ordered_networks[nsq]]}\")\n",
    "\n",
    "    max_courant = 0\n",
    "    maxa = []\n",
    "    for result in results:\n",
    "        for seg in result:\n",
    "            maxa.extend(result[seg][:, 8:9])\n",
    "    max_courant = max(maxa)\n",
    "    print(f\"max_courant: {max_courant}\")\n",
    "\n",
    "    if (\n",
    "        nsq > 0\n",
    "    ):  # We skip this step for zero-order networks, i.e., those that have no downstream dependents\n",
    "        flowveldepth_connect = (\n",
    "            {}\n",
    "        )  # There is no need to preserve previously passed on values -- so we clear the dictionary\n",
    "        for i, (terminal_segment, network) in enumerate(ordered_networks[nsq]):\n",
    "            # seg = network[\"reaches\"][network[\"terminal_reach\"]][\"reach_tail\"]\n",
    "            seg = terminal_segment\n",
    "            flowveldepth_connect[seg] = {}\n",
    "            flowveldepth_connect[seg] = results[i][seg]\n",
    "            # TODO: The value passed here could be much more specific to\n",
    "            # TODO: exactly and only the most recent time step for the passing reach\n",
    "\n",
    "if parallel_compute:\n",
    "    pool.close()\n",
    "\n",
    "if percentage_complete:\n",
    "    pbar.close()\n",
    "\n",
    "if verbose:\n",
    "    print(\"ordered reach computation complete\")\n",
    "if showtiming:\n",
    "    print(\"... in %s seconds.\" % (time.time() - main_start_time))\n",
    "if verbose:\n",
    "    print(\"program complete\")\n",
    "if showtiming:\n",
    "    print(\"... in %s seconds.\" % (time.time() - program_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the main dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results)\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "all_results = {}\n",
    "seg_courant_maxes = []\n",
    "time = []\n",
    "flowval = []  # flowval\n",
    "velval_list = []  # velval\n",
    "depthval = []  # depthval\n",
    "qlatval = []  # qlatval\n",
    "storageval = []  # storageval\n",
    "qlatCumval = []  # qlatCumval\n",
    "kinCelerity = []  # ck\n",
    "courant = []  # cn\n",
    "X = []  # X\n",
    "\n",
    "for result in results:\n",
    "    all_results.update(result)\n",
    "\n",
    "# print(all_results[8780801][1][0])\n",
    "\n",
    "df = pd.DataFrame()\n",
    "# print(all_results)\n",
    "\n",
    "for key, data in all_results.items():\n",
    "    time.extend(data[:, 0])  # time\n",
    "    flowval.extend(data[:, 1])  # flowval\n",
    "    velval_list.extend(data[:, 2])  # velval\n",
    "    depthval.extend(data[:, 3])  # depthval\n",
    "    qlatval.extend(data[:, 4])  # qlatval\n",
    "    storageval.extend(data[:, 5])  # storageval\n",
    "    qlatCumval.extend(data[:, 6])  # qlatCumval\n",
    "    kinCelerity.extend(data[:, 7])  # ck\n",
    "    courant.extend(data[:, 8])  # cn\n",
    "    X.extend(data[:, 9])  # X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_seg_length = len(all_results[933020089][:, 0])\n",
    "single_seg_length * 611\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_index = all_results.keys()\n",
    "key_index = list(\n",
    "    itertools.chain.from_iterable(\n",
    "        itertools.repeat(x, single_seg_length) for x in key_index\n",
    "    )\n",
    ")\n",
    "data = {\n",
    "    \"key_index\": list(key_index),\n",
    "    \"time\": list(time),\n",
    "    \"flowval\": list(flowval),\n",
    "    \"velval_list\": list(velval_list),\n",
    "    \"depthval\": list(depthval),\n",
    "    \"qlatval\": list(qlatval),\n",
    "    \"storageval\": list(storageval),\n",
    "    \"qlatCumval\": list(qlatCumval),\n",
    "    \"kinCelerity\": list(kinCelerity),\n",
    "    \"courant\": list(courant),\n",
    "    \"X\": list(X),\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df = df.set_index(\"key_index\")\n",
    "df = df.reset_index()\n",
    "df = df.set_index(\"key_index\")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 25 courants based on our work this morning and their segment IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list = []\n",
    "seg_courant_maxes = []\n",
    "for seg in all_results:\n",
    "    seg_list.append(seg)\n",
    "    seg_courant_maxes.append(max(all_results[seg][:, tr.courant_index]))\n",
    "zipped = zip(seg_list, seg_courant_maxes)\n",
    "zipped = list(zipped)\n",
    "res = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
    "# A.sort(reverse=True)\n",
    "res = (res)[:25]\n",
    "print((res)[:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of those IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_segments = []\n",
    "for x, y in res[:25]:\n",
    "    major_segments.append(x)\n",
    "major_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out the above segment IDs from the original df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.loc[df.index.isin(major_segments), :]\n",
    "df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"courant\"] == 25.363832473754883]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated plotter based on filtered dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot\n",
    "import plotly.io as pio\n",
    "\n",
    "df_indexed = df_filtered.reset_index()\n",
    "test_chart = go.FigureWidget()\n",
    "for x in range(0, 25):\n",
    "    temp_df_range_1 = single_seg_length * (x)\n",
    "    temp_df_range_2 = single_seg_length * (x + 1)\n",
    "    print(temp_df_range_1, temp_df_range_2)\n",
    "    test_chart.add_scatter(\n",
    "        x=df_indexed[temp_df_range_1:temp_df_range_2][\"time\"],\n",
    "        y=df_indexed[temp_df_range_1:temp_df_range_2][\"flowval\"],\n",
    "        name=\"segment \" + str(df_indexed[\"key_index\"][temp_df_range_1]),\n",
    "    )\n",
    "test_chart.layout.title = \"Timestep Chart \" + str(dt)\n",
    "plot(test_chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_input_folder = root.joinpath(\"test\", \"input\", \"yaml\")\n",
    "custom_input_file = \"florence_933020089_dt300.yaml\"\n",
    "all_results = tr.route_supernetwork(\n",
    "    tr.connections_g,\n",
    "    tr.networks_g,\n",
    "    tr.qlateral_g,\n",
    "    tr.waterbodies_df_g,\n",
    "    tr.waterbody_initial_states_df_g,\n",
    "    tr.channel_initial_states_df_g,\n",
    "    custom_input_file=custom_input_folder.joinpath(custom_input_file),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_input_file_list = [\"florence_933020089_dt300.yaml\",\"florence_933020089_dt60.yaml\"]\n",
    "df_indexed_300 = pd.DataFrame()\n",
    "df_indexed_60 = pd.DataFrame()\n",
    "for c_i in (custom_input_file_list):\n",
    "    print(c_i)\n",
    "    import pathlib\n",
    "    import sys\n",
    "    import time\n",
    "    import glob\n",
    "    from tqdm import tqdm\n",
    "    import multiprocessing\n",
    "\n",
    "    root = pathlib.Path(\"../../\").resolve()\n",
    "    sys.path.append(str(root.joinpath(\"src\", \"python_framework_v02\")))\n",
    "    sys.path.append(str(root.joinpath(\"src\", \"python_framework_v01\")))\n",
    "    sys.path.append(str(root.joinpath(\"src\", \"python_routing_v01\")))\n",
    "    sys.path.append(\".\")\n",
    "    import nhd_io as nio\n",
    "    import compute_nhd_routing_SingleSeg as tr\n",
    "    import nhd_network_utilities_v01 as nnu\n",
    "    import nhd_reach_utilities as nru\n",
    "\n",
    "    custom_input_folder = root.joinpath(\"test\", \"input\", \"yaml\")\n",
    "    custom_input_file = c_i\n",
    "    run_pocono2_test = None\n",
    "\n",
    "    supernetwork_parameters = None\n",
    "    waterbody_parameters = None\n",
    "    if custom_input_file:\n",
    "        (\n",
    "            supernetwork_parameters,\n",
    "            waterbody_parameters,\n",
    "            forcing_parameters,\n",
    "            restart_parameters,\n",
    "            output_parameters,\n",
    "            run_parameters,\n",
    "        ) = nio.read_custom_input(custom_input_folder.joinpath(custom_input_file))\n",
    "\n",
    "        break_network_at_waterbodies = run_parameters.get(\n",
    "            \"break_network_at_waterbodies\", None\n",
    "        )\n",
    "\n",
    "        dt = run_parameters.get(\"dt\", None)\n",
    "        nts = run_parameters.get(\"nts\", None)\n",
    "        qts_subdivisions = run_parameters.get(\"qts_subdivisions\", None)\n",
    "        debuglevel = -1 * int(run_parameters.get(\"debuglevel\", 0))\n",
    "        verbose = run_parameters.get(\"verbose\", None)\n",
    "        showtiming = run_parameters.get(\"showtiming\", None)\n",
    "        percentage_complete = run_parameters.get(\"percentage_complete\", None)\n",
    "        do_network_analysis_only = run_parameters.get(\"do_network_analysis_only\", None)\n",
    "        assume_short_ts = run_parameters.get(\"assume_short_ts\", None)\n",
    "        parallel_compute = run_parameters.get(\"parallel_compute\", None)\n",
    "        cpu_pool = run_parameters.get(\"cpu_pool\", None)\n",
    "        sort_networks = run_parameters.get(\"sort_networks\", None)\n",
    "\n",
    "        csv_output = output_parameters.get(\"csv_output\", None)\n",
    "        nc_output_folder = output_parameters.get(\"nc_output_folder\", None)\n",
    "\n",
    "        qlat_const = forcing_parameters.get(\"qlat_const\", None)\n",
    "        qlat_input_file = forcing_parameters.get(\"qlat_input_file\", None)\n",
    "        qlat_input_folder = forcing_parameters.get(\"qlat_input_folder\", None)\n",
    "        qlat_file_pattern_filter = forcing_parameters.get(\"qlat_file_pattern_filter\", None)\n",
    "        qlat_file_index_col = forcing_parameters.get(\"qlat_file_index_col\", None)\n",
    "        qlat_file_value_col = forcing_parameters.get(\"qlat_file_value_col\", None)\n",
    "\n",
    "        wrf_hydro_channel_restart_file = restart_parameters.get(\n",
    "            \"wrf_hydro_channel_restart_file\", None\n",
    "        )\n",
    "        wrf_hydro_channel_ID_crosswalk_file = restart_parameters.get(\n",
    "            \"wrf_hydro_channel_ID_crosswalk_file\", None\n",
    "        )\n",
    "        wrf_hydro_channel_ID_crosswalk_file_field_name = restart_parameters.get(\n",
    "            \"wrf_hydro_channel_ID_crosswalk_file_field_name\", None\n",
    "        )\n",
    "        wrf_hydro_channel_restart_upstream_flow_field_name = restart_parameters.get(\n",
    "            \"wrf_hydro_channel_restart_upstream_flow_field_name\", None\n",
    "        )\n",
    "        wrf_hydro_channel_restart_downstream_flow_field_name = restart_parameters.get(\n",
    "            \"wrf_hydro_channel_restart_downstream_flow_field_name\", None\n",
    "        )\n",
    "        wrf_hydro_channel_restart_depth_flow_field_name = restart_parameters.get(\n",
    "            \"wrf_hydro_channel_restart_depth_flow_field_name\", None\n",
    "        )\n",
    "\n",
    "        wrf_hydro_waterbody_restart_file = restart_parameters.get(\n",
    "            \"wrf_hydro_waterbody_restart_file\", None\n",
    "        )\n",
    "        wrf_hydro_waterbody_ID_crosswalk_file = restart_parameters.get(\n",
    "            \"wrf_hydro_waterbody_ID_crosswalk_file\", None\n",
    "        )\n",
    "        wrf_hydro_waterbody_ID_crosswalk_file_field_name = restart_parameters.get(\n",
    "            \"wrf_hydro_waterbody_ID_crosswalk_file_field_name\", None\n",
    "        )\n",
    "        wrf_hydro_waterbody_crosswalk_filter_file = restart_parameters.get(\n",
    "            \"wrf_hydro_waterbody_crosswalk_filter_file\", None\n",
    "        )\n",
    "        wrf_hydro_waterbody_crosswalk_filter_file_field_name = restart_parameters.get(\n",
    "            \"wrf_hydro_waterbody_crosswalk_filter_file_field_name\", None\n",
    "        )\n",
    "\n",
    "    # Any specific commandline arguments will override the file\n",
    "    # TODO: There are probably some pathological collisions that could\n",
    "    # arise from this ordering ... check these out.\n",
    "\n",
    "    if run_pocono2_test:\n",
    "        if verbose:\n",
    "            print(\"running test case for Pocono_TEST2 domain\")\n",
    "        # Overwrite the following test defaults\n",
    "        supernetwork = \"Pocono_TEST2\"\n",
    "        break_network_at_waterbodies = False\n",
    "        qts_subdivisions = 1  # change qts_subdivisions = 1 as  default\n",
    "        dt = 300 / qts_subdivisions\n",
    "        nts = 144 * qts_subdivisions\n",
    "        csv_output = {\"csv_output_folder\": os.path.join(root, \"test\", \"output\", \"text\")}\n",
    "        nc_output_folder = os.path.join(root, \"test\", \"output\", \"text\")\n",
    "        # test 1. Take lateral flow from re-formatted wrf-hydro output from Pocono Basin simulation\n",
    "        qlat_input_file = os.path.join(\n",
    "            root, r\"test/input/geo/PoconoSampleData2/Pocono_ql_testsamp1_nwm_mc.csv\"\n",
    "        )\n",
    "    if showtiming:\n",
    "        program_start_time = time.time()\n",
    "    if verbose:\n",
    "        print(f\"begin program t-route ...\")\n",
    "\n",
    "    # STEP 1: Read the supernetwork dataset and build the connections graph\n",
    "    if verbose:\n",
    "        print(\"creating supernetwork connections set\")\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "\n",
    "    if supernetwork_parameters:\n",
    "        supernetwork_values = nnu.get_nhd_connections(\n",
    "            supernetwork_parameters=supernetwork_parameters,\n",
    "            verbose=False,\n",
    "            debuglevel=debuglevel,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        test_folder = os.path.join(root, r\"test\")\n",
    "        geo_input_folder = os.path.join(test_folder, r\"input\", r\"geo\")\n",
    "        supernetwork_parameters, supernetwork_values = nnu.set_networks(\n",
    "            supernetwork=supernetwork,\n",
    "            geo_input_folder=geo_input_folder,\n",
    "            verbose=False,\n",
    "            debuglevel=debuglevel,\n",
    "        )\n",
    "        waterbody_parameters = nnu.set_waterbody_parameters(\n",
    "            supernetwork=supernetwork,\n",
    "            geo_input_folder=geo_input_folder,\n",
    "            verbose=False,\n",
    "            debuglevel=debuglevel,\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"supernetwork connections set complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "    connections = supernetwork_values[0]\n",
    "    # STEP 2: Separate the networks and build the sub-graph of reaches within each network\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"organizing connections into reaches ...\")\n",
    "    networks = nru.compose_networks(\n",
    "        supernetwork_values,\n",
    "        break_network_at_waterbodies=break_network_at_waterbodies,\n",
    "        verbose=False,\n",
    "        debuglevel=debuglevel,\n",
    "        showtiming=showtiming,\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"reach organization complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "    # STEP 3: Organize Network for Waterbodies\n",
    "    if break_network_at_waterbodies:\n",
    "        if showtiming:\n",
    "            start_time = time.time()\n",
    "        if verbose:\n",
    "            print(\"reading waterbody parameter file ...\")\n",
    "\n",
    "        ## STEP 3a: Read waterbody parameter file\n",
    "        waterbodies_values = supernetwork_values[12]\n",
    "        waterbodies_segments = supernetwork_values[13]\n",
    "        connections_tailwaters = supernetwork_values[4]\n",
    "\n",
    "        waterbodies_df = nio.read_waterbody_df(waterbody_parameters, waterbodies_values,)\n",
    "        waterbodies_df = waterbodies_df.sort_index(axis=\"index\").sort_index(axis=\"columns\")\n",
    "\n",
    "        nru.order_networks(connections, networks, connections_tailwaters)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"waterbodies complete\")\n",
    "        if showtiming:\n",
    "            print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "\n",
    "        ## STEP 3b: Order subnetworks above and below reservoirs\n",
    "        if showtiming:\n",
    "            start_time = time.time()\n",
    "        if verbose:\n",
    "            print(\"ordering waterbody subnetworks ...\")\n",
    "\n",
    "        max_network_seqorder = -1\n",
    "        for network in networks:\n",
    "            max_network_seqorder = max(\n",
    "                networks[network][\"network_seqorder\"], max_network_seqorder\n",
    "            )\n",
    "        ordered_networks = {}\n",
    "\n",
    "        for terminal_segment, network in networks.items():\n",
    "            if network[\"network_seqorder\"] not in ordered_networks:\n",
    "                ordered_networks[network[\"network_seqorder\"]] = []\n",
    "            ordered_networks[network[\"network_seqorder\"]].append(\n",
    "                (terminal_segment, network)\n",
    "            )\n",
    "\n",
    "        if verbose:\n",
    "            print(\"ordering waterbody subnetworks complete\")\n",
    "        if showtiming:\n",
    "            print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "\n",
    "    else:\n",
    "        # If we are not splitting the networks, we can put them all in one order\n",
    "        max_network_seqorder = 0\n",
    "        ordered_networks = {}\n",
    "        ordered_networks[0] = [\n",
    "            (terminal_segment, network) for terminal_segment, network in networks.items()\n",
    "        ]\n",
    "\n",
    "    if do_network_analysis_only:\n",
    "        sys.exit()\n",
    "\n",
    "    if break_network_at_waterbodies:\n",
    "        ## STEP 3c: Handle Waterbody Initial States\n",
    "        if showtiming:\n",
    "            start_time = time.time()\n",
    "        if verbose:\n",
    "            print(\"setting waterbody initial states ...\")\n",
    "\n",
    "        if wrf_hydro_waterbody_restart_file:\n",
    "\n",
    "            waterbody_initial_states_df = nio.get_reservoir_restart_from_wrf_hydro(\n",
    "                wrf_hydro_waterbody_restart_file,\n",
    "                wrf_hydro_waterbody_ID_crosswalk_file,\n",
    "                wrf_hydro_waterbody_ID_crosswalk_file_field_name,\n",
    "                wrf_hydro_waterbody_crosswalk_filter_file,\n",
    "                wrf_hydro_waterbody_crosswalk_filter_file_field_name,\n",
    "            )\n",
    "        else:\n",
    "            # TODO: Consider adding option to read cold state from route-link file\n",
    "            waterbody_initial_ds_flow_const = 0.0\n",
    "            waterbody_initial_depth_const = 0.0\n",
    "            # Set initial states from cold-state\n",
    "            waterbody_initial_states_df = pd.DataFrame(\n",
    "                0, index=waterbodies_df.index, columns=[\"qd0\", \"h0\",], dtype=\"float32\"\n",
    "            )\n",
    "            # TODO: This assignment could probably by done in the above call\n",
    "            waterbody_initial_states_df[\"qd0\"] = waterbody_initial_ds_flow_const\n",
    "            waterbody_initial_states_df[\"h0\"] = waterbody_initial_depth_const\n",
    "            waterbody_initial_states_df[\"index\"] = range(len(waterbody_initial_states_df))\n",
    "\n",
    "        if verbose:\n",
    "            print(\"waterbody initial states complete\")\n",
    "        if showtiming:\n",
    "            print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "    # STEP 4: Handle Channel Initial States\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"setting channel initial states ...\")\n",
    "\n",
    "    if wrf_hydro_channel_restart_file:\n",
    "\n",
    "        channel_initial_states_df = nio.get_stream_restart_from_wrf_hydro(\n",
    "            wrf_hydro_channel_restart_file,\n",
    "            wrf_hydro_channel_ID_crosswalk_file,\n",
    "            wrf_hydro_channel_ID_crosswalk_file_field_name,\n",
    "            wrf_hydro_channel_restart_upstream_flow_field_name,\n",
    "            wrf_hydro_channel_restart_downstream_flow_field_name,\n",
    "            wrf_hydro_channel_restart_depth_flow_field_name,\n",
    "        )\n",
    "    else:\n",
    "        # TODO: Consider adding option to read cold state from route-link file\n",
    "        channel_initial_us_flow_const = 0.0\n",
    "        channel_initial_ds_flow_const = 0.0\n",
    "        channel_initial_depth_const = 0.0\n",
    "        # Set initial states from cold-state\n",
    "        channel_initial_states_df = pd.DataFrame(\n",
    "            0, index=connections.keys(), columns=[\"qu0\", \"qd0\", \"h0\",], dtype=\"float32\"\n",
    "        )\n",
    "        channel_initial_states_df[\"qu0\"] = channel_initial_us_flow_const\n",
    "        channel_initial_states_df[\"qd0\"] = channel_initial_ds_flow_const\n",
    "        channel_initial_states_df[\"h0\"] = channel_initial_depth_const\n",
    "        channel_initial_states_df[\"index\"] = range(len(channel_initial_states_df))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"channel initial states complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "    # STEP 5: Read (or set) QLateral Inputs\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"creating qlateral array ...\")\n",
    "\n",
    "    # initialize qlateral dict\n",
    "    qlateral = {}\n",
    "\n",
    "    if qlat_input_folder:\n",
    "        qlat_files = []\n",
    "        for pattern in qlat_file_pattern_filter:\n",
    "            qlat_files.extend(glob.glob(qlat_input_folder + pattern))\n",
    "        qlat_df = nio.get_ql_from_wrf_hydro(\n",
    "            qlat_files=qlat_files,\n",
    "            index_col=qlat_file_index_col,\n",
    "            value_col=qlat_file_value_col,\n",
    "        )\n",
    "\n",
    "    elif qlat_input_file:\n",
    "        qlat_df = nio.get_ql_from_csv(qlat_input_file)\n",
    "\n",
    "    else:\n",
    "        qlat_df = pd.DataFrame(\n",
    "            qlat_const, index=connections.keys(), columns=range(nts), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "    for index, row in qlat_df.iterrows():\n",
    "        qlateral[index] = row.tolist()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"qlateral array complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "    # STEP 6: Sort the ordered networks\n",
    "    if sort_networks:\n",
    "        if showtiming:\n",
    "            start_time = time.time()\n",
    "        if verbose:\n",
    "            print(\"sorting the ordered networks ...\")\n",
    "\n",
    "        for nsq in range(max_network_seqorder, -1, -1):\n",
    "            sort_ordered_network(ordered_networks[nsq], True)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"sorting complete\")\n",
    "        if showtiming:\n",
    "            print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "    # Define the pool after we create the static global objects (and collect the garbage)\n",
    "    if parallel_compute:\n",
    "        import gc\n",
    "\n",
    "        gc.collect()\n",
    "        pool = multiprocessing.Pool(cpu_pool)\n",
    "\n",
    "    flowveldepth_connect = (\n",
    "        {}\n",
    "    )  # dict to contain values to transfer from upstream to downstream networks\n",
    "    ################### Main Execution Loop across ordered networks\n",
    "    if showtiming:\n",
    "        main_start_time = time.time()\n",
    "    if verbose:\n",
    "        print(f\"executing routing computation ...\")\n",
    "\n",
    "    compute_network_func = tr.compute_network\n",
    "\n",
    "    tr.connections_g = connections\n",
    "    tr.networks_g = networks\n",
    "    tr.qlateral_g = qlateral\n",
    "    tr.waterbodies_df_g = waterbodies_df\n",
    "    tr.waterbody_initial_states_df_g = waterbody_initial_states_df\n",
    "    tr.channel_initial_states_df_g = channel_initial_states_df\n",
    "\n",
    "    progress_count = 0\n",
    "    percentage_complete = True\n",
    "    if percentage_complete:\n",
    "        for nsq in range(max_network_seqorder, -1, -1):\n",
    "            for terminal_segment, network in ordered_networks[nsq]:\n",
    "                progress_count += len(network[\"all_segments\"])\n",
    "        pbar = tqdm(total=(progress_count))\n",
    "\n",
    "    for nsq in range(max_network_seqorder, -1, -1):\n",
    "\n",
    "        if parallel_compute:\n",
    "            nslist = []\n",
    "        results = []\n",
    "\n",
    "        current_index_total = 0\n",
    "\n",
    "        for terminal_segment, network in ordered_networks[nsq]:\n",
    "\n",
    "            if percentage_complete:\n",
    "                if current_index_total == 0:\n",
    "                    pbar.update(0)\n",
    "\n",
    "            if break_network_at_waterbodies:\n",
    "                waterbody = waterbodies_segments.get(terminal_segment)\n",
    "            else:\n",
    "                waterbody = None\n",
    "            if not parallel_compute:  # serial execution\n",
    "                if showtiming:\n",
    "                    start_time = time.time()\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"routing ordered reaches for terminal segment {terminal_segment} ...\"\n",
    "                    )\n",
    "\n",
    "                results.append(\n",
    "                    compute_network_func(\n",
    "                        flowveldepth_connect=flowveldepth_connect,\n",
    "                        terminal_segment=terminal_segment,\n",
    "                        supernetwork_parameters=supernetwork_parameters,\n",
    "                        waterbody_parameters=waterbody_parameters,\n",
    "                        waterbody=waterbody,\n",
    "                        nts=nts,\n",
    "                        dt=dt,\n",
    "                        qts_subdivisions=qts_subdivisions,\n",
    "                        verbose=verbose,\n",
    "                        debuglevel=debuglevel,\n",
    "                        csv_output=csv_output,\n",
    "                        nc_output_folder=nc_output_folder,\n",
    "                        assume_short_ts=assume_short_ts,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                if showtiming:\n",
    "                    print(\"... complete in %s seconds.\" % (time.time() - start_time))\n",
    "                if percentage_complete:\n",
    "                    pbar.update(len(network[\"all_segments\"]))\n",
    "\n",
    "            else:  # parallel execution\n",
    "                nslist.append(\n",
    "                    [\n",
    "                        flowveldepth_connect,\n",
    "                        terminal_segment,\n",
    "                        supernetwork_parameters,  # TODO: This should probably be global...\n",
    "                        waterbody_parameters,\n",
    "                        waterbody,\n",
    "                        nts,\n",
    "                        dt,\n",
    "                        qts_subdivisions,\n",
    "                        verbose,\n",
    "                        debuglevel,\n",
    "                        csv_output,\n",
    "                        nc_output_folder,\n",
    "                        assume_short_ts,\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "        if parallel_compute:\n",
    "            if verbose:\n",
    "                print(f\"routing ordered reaches for networks of order {nsq} ... \")\n",
    "            if debuglevel <= -2:\n",
    "                print(f\"reaches to be routed include:\")\n",
    "                print(f\"{[network[0] for network in ordered_networks[nsq]]}\")\n",
    "            # with pool:\n",
    "            # with multiprocessing.Pool() as pool:\n",
    "            results = pool.starmap(compute_network_func, nslist)\n",
    "\n",
    "            if showtiming:\n",
    "                print(\"... complete in %s seconds.\" % (time.time() - start_time))\n",
    "            if percentage_complete:\n",
    "                pbar.update(\n",
    "                    sum(\n",
    "                        len(network[1][\"all_segments\"]) for network in ordered_networks[nsq]\n",
    "                    )\n",
    "                )\n",
    "                # print(f\"{[network[0] for network in ordered_networks[nsq]]}\")\n",
    "\n",
    "        max_courant = 0\n",
    "        maxa = []\n",
    "        for result in results:\n",
    "            for seg in result:\n",
    "                maxa.extend(result[seg][:, 8:9])\n",
    "        max_courant = max(maxa)\n",
    "        print(f\"max_courant: {max_courant}\")\n",
    "\n",
    "        if (\n",
    "            nsq > 0\n",
    "        ):  # We skip this step for zero-order networks, i.e., those that have no downstream dependents\n",
    "            flowveldepth_connect = (\n",
    "                {}\n",
    "            )  # There is no need to preserve previously passed on values -- so we clear the dictionary\n",
    "            for i, (terminal_segment, network) in enumerate(ordered_networks[nsq]):\n",
    "                # seg = network[\"reaches\"][network[\"terminal_reach\"]][\"reach_tail\"]\n",
    "                seg = terminal_segment\n",
    "                flowveldepth_connect[seg] = {}\n",
    "                flowveldepth_connect[seg] = results[i][seg]\n",
    "                # TODO: The value passed here could be much more specific to\n",
    "                # TODO: exactly and only the most recent time step for the passing reach\n",
    "\n",
    "    if parallel_compute:\n",
    "        pool.close()\n",
    "\n",
    "    if percentage_complete:\n",
    "        pbar.close()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"ordered reach computation complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - main_start_time))\n",
    "    if verbose:\n",
    "        print(\"program complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - program_start_time))\n",
    "    \n",
    "    \n",
    "    \n",
    "    all_results_new = {}\n",
    "    seg_courant_maxes = []\n",
    "    time = []\n",
    "    flowval = []  # flowval\n",
    "    velval_list = []  # velval\n",
    "    depthval = []  # depthval\n",
    "    qlatval = []  # qlatval\n",
    "    storageval = []  # storageval\n",
    "    qlatCumval = []  # qlatCumval\n",
    "    kinCelerity = []  # ck\n",
    "    courant = []  # cn\n",
    "    X = []  # X\n",
    "\n",
    "    for result in results:\n",
    "        all_results_new.update(result)\n",
    "\n",
    "    # print(all_results[8780801][1][0])\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    # print(all_results)\n",
    "\n",
    "    for key, data in all_results_new.items():\n",
    "        time.extend(data[:, 0])  # time\n",
    "        flowval.extend(data[:, 1])  # flowval\n",
    "        velval_list.extend(data[:, 2])  # velval\n",
    "        depthval.extend(data[:, 3])  # depthval\n",
    "        qlatval.extend(data[:, 4])  # qlatval\n",
    "        storageval.extend(data[:, 5])  # storageval\n",
    "        qlatCumval.extend(data[:, 6])  # qlatCumval\n",
    "        kinCelerity.extend(data[:, 7])  # ck\n",
    "        courant.extend(data[:, 8])  # cn\n",
    "        X.extend(data[:, 9])  # X\n",
    "#     print(all_results_new)\n",
    "    single_seg_length = len(all_results_new[933020089][:, 0])\n",
    "\n",
    "    if c_i == \"florence_933020089_dt300.yaml\":\n",
    "        single_seg_length_300 = single_seg_length\n",
    "    else:\n",
    "        single_seg_length_60 = single_seg_length\n",
    "#     else:\n",
    "#         single_seg_length_10 = single_seg_length\n",
    "    print(single_seg_length_300,single_seg_length_60)\n",
    "    # single_seg_length * 611\n",
    "\n",
    "    key_index = all_results.keys()\n",
    "    key_index = list(\n",
    "        itertools.chain.from_iterable(\n",
    "            itertools.repeat(x, single_seg_length) for x in key_index\n",
    "        )\n",
    "    )\n",
    "    data = {\n",
    "        \"key_index\": list(key_index),\n",
    "        \"time\": list(time),\n",
    "        \"flowval\": list(flowval),\n",
    "        \"velval_list\": list(velval_list),\n",
    "        \"depthval\": list(depthval),\n",
    "        \"qlatval\": list(qlatval),\n",
    "        \"storageval\": list(storageval),\n",
    "        \"qlatCumval\": list(qlatCumval),\n",
    "        \"kinCelerity\": list(kinCelerity),\n",
    "        \"courant\": list(courant),\n",
    "        \"X\": list(X),\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.set_index(\"key_index\")\n",
    "    df = df.reset_index()\n",
    "    df = df.set_index(\"key_index\")\n",
    "\n",
    "\n",
    "    seg_list = []\n",
    "    seg_courant_maxes = []\n",
    "    for seg in all_results_new:\n",
    "        seg_list.append(seg)\n",
    "        seg_courant_maxes.append(max(all_results_new[seg][:, tr.courant_index]))\n",
    "    zipped = zip(seg_list, seg_courant_maxes)\n",
    "    zipped = list(zipped)\n",
    "    res = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
    "    # A.sort(reverse=True)\n",
    "    res = (res)[:3]\n",
    "    # print((res)[:25])\n",
    "\n",
    "    major_segments = []\n",
    "    for x, y in res[:3]:\n",
    "        major_segments.append(x)\n",
    "    # major_segments\n",
    "\n",
    "    df_filtered = df.loc[df.index.isin(major_segments), :]\n",
    "\n",
    "    if c_i == \"florence_933020089_dt300.yaml\":\n",
    "        df_indexed_300 = df_filtered.reset_index()\n",
    "    else:\n",
    "        df_indexed_60 = df_filtered.reset_index()\n",
    "    print(df_indexed_300,df_indexed_60)\n",
    "#     else:\n",
    "#         df_indexed_10 = df_filtered.reset_index()\n",
    "\n",
    "test_chart = go.FigureWidget()\n",
    "for x in range(0,len(major_segments)):\n",
    "    temp_df_range_1 = single_seg_length_300 * (x)\n",
    "    temp_df_range_2 = single_seg_length_300 * (x + 1)\n",
    "    #         print(temp_df_range_1, temp_df_range_2)\n",
    "    test_chart.add_scatter(\n",
    "        x=df_indexed_300[temp_df_range_1:temp_df_range_2][\"time\"],\n",
    "        y=df_indexed_300[temp_df_range_1:temp_df_range_2][\"flowval\"],\n",
    "        name=\"segment flowval \" + str(df_indexed_300[\"key_index\"][temp_df_range_1]) + \" \" + str(300),\n",
    "    )\n",
    "    temp_df_range_1 = single_seg_length_60 * (x)\n",
    "    temp_df_range_2 = single_seg_length_60 * (x + 1)\n",
    "    #         print(temp_df_range_1, temp_df_range_2)\n",
    "    test_chart.add_scatter(\n",
    "        x=df_indexed_60[temp_df_range_1:temp_df_range_2][\"time\"],\n",
    "        y=df_indexed_60[temp_df_range_1:temp_df_range_2][\"flowval\"],\n",
    "        name=\"segment flowval \" + str(df_indexed_60[\"key_index\"][temp_df_range_1]) + \" \" + str(60),\n",
    "    )\n",
    "    temp_df_range_1 = single_seg_length_300 * (x)\n",
    "    temp_df_range_2 = single_seg_length_300 * (x + 1)\n",
    "    test_chart.add_scatter(\n",
    "        x=df_indexed_300[temp_df_range_1:temp_df_range_2][\"time\"],\n",
    "        y=df_indexed_300[temp_df_range_1:temp_df_range_2][\"courant\"],\n",
    "        name=\"segment courant \" + str(df_indexed_300[\"key_index\"][temp_df_range_1]) + \" \" + str(300),\n",
    "    )\n",
    "    temp_df_range_1 = single_seg_length_60 * (x)\n",
    "    temp_df_range_2 = single_seg_length_60 * (x + 1)\n",
    "    #         print(temp_df_range_1, temp_df_range_2)\n",
    "    test_chart.add_scatter(\n",
    "        x=df_indexed_60[temp_df_range_1:temp_df_range_2][\"time\"],\n",
    "        y=df_indexed_60[temp_df_range_1:temp_df_range_2][\"courant\"],\n",
    "        name=\"segment courant \" + str(df_indexed_60[\"key_index\"][temp_df_range_1]) + \" \" + str(60),\n",
    "    )\n",
    "\n",
    "test_chart.layout.title = \"Timestep Chart 300, 60\"\n",
    "plot(test_chart)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_seg_length_300,single_seg_length_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_indexed_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_indexed_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_input_file_list = [\"florence_933020089_dt300.yaml\",\"florence_933020089_dt60.yaml\"]\n",
    "# # custom_input_file_list = [\"florence_933020089_dt300.yaml\"]\n",
    "# df_indexed_300 = pd.DataFrame()\n",
    "# df_indexed_60 = pd.DataFrame()\n",
    "# # df_indexed_10 = pd.DataFrame()\n",
    "# single_seg_length_300 = 0\n",
    "# single_seg_length_60 = 0\n",
    "# # single_seg_length_10 = 0\n",
    "# for i,c_i in enumerate(custom_input_file_list):\n",
    "#     print(c_i)\n",
    "#     custom_input_folder = root.joinpath(\"test\", \"input\", \"yaml\")\n",
    "#     custom_input_file = c_i\n",
    "#     all_results_new = tr.route_supernetwork(\n",
    "#         tr.connections_g,\n",
    "#         tr.networks_g,\n",
    "#         tr.qlateral_g,\n",
    "#         tr.waterbodies_df_g,\n",
    "#         tr.waterbody_initial_states_df_g,\n",
    "#         tr.channel_initial_states_df_g,\n",
    "#         custom_input_file=custom_input_folder.joinpath(custom_input_file),\n",
    "#     )\n",
    "    \n",
    "\n",
    "    \n",
    "#     print(len(all_results_new))\n",
    "#     all_results_new = {}\n",
    "#     seg_courant_maxes = []\n",
    "#     time = []\n",
    "#     flowval = []  # flowval\n",
    "#     velval_list = []  # velval\n",
    "#     depthval = []  # depthval\n",
    "#     qlatval = []  # qlatval\n",
    "#     storageval = []  # storageval\n",
    "#     qlatCumval = []  # qlatCumval\n",
    "#     kinCelerity = []  # ck\n",
    "#     courant = []  # cn\n",
    "#     X = []  # X\n",
    "\n",
    "#     for result in results:\n",
    "#         all_results_new.update(result)\n",
    "\n",
    "#     # print(all_results[8780801][1][0])\n",
    "\n",
    "#     df = pd.DataFrame()\n",
    "#     # print(all_results)\n",
    "\n",
    "#     for key, data in all_results_new.items():\n",
    "#         time.extend(data[:, 0])  # time\n",
    "#         flowval.extend(data[:, 1])  # flowval\n",
    "#         velval_list.extend(data[:, 2])  # velval\n",
    "#         depthval.extend(data[:, 3])  # depthval\n",
    "#         qlatval.extend(data[:, 4])  # qlatval\n",
    "#         storageval.extend(data[:, 5])  # storageval\n",
    "#         qlatCumval.extend(data[:, 6])  # qlatCumval\n",
    "#         kinCelerity.extend(data[:, 7])  # ck\n",
    "#         courant.extend(data[:, 8])  # cn\n",
    "#         X.extend(data[:, 9])  # X\n",
    "# #     print(all_results_new)\n",
    "#     single_seg_length = len(all_results_new[933020089][:, 0])\n",
    "#     print(single_seg_length)\n",
    "#     if i == 0:\n",
    "#         single_seg_length_300 = single_seg_length\n",
    "#     elif i == 1:\n",
    "#         single_seg_length_60 = single_seg_length\n",
    "# #     else:\n",
    "# #         single_seg_length_10 = single_seg_length\n",
    "\n",
    "#     # single_seg_length * 611\n",
    "\n",
    "#     key_index = all_results.keys()\n",
    "#     key_index = list(\n",
    "#         itertools.chain.from_iterable(\n",
    "#             itertools.repeat(x, single_seg_length) for x in key_index\n",
    "#         )\n",
    "#     )\n",
    "#     data = {\n",
    "#         \"key_index\": list(key_index),\n",
    "#         \"time\": list(time),\n",
    "#         \"flowval\": list(flowval),\n",
    "#         \"velval_list\": list(velval_list),\n",
    "#         \"depthval\": list(depthval),\n",
    "#         \"qlatval\": list(qlatval),\n",
    "#         \"storageval\": list(storageval),\n",
    "#         \"qlatCumval\": list(qlatCumval),\n",
    "#         \"kinCelerity\": list(kinCelerity),\n",
    "#         \"courant\": list(courant),\n",
    "#         \"X\": list(X),\n",
    "#     }\n",
    "#     df = pd.DataFrame(data)\n",
    "#     df = df.set_index(\"key_index\")\n",
    "#     df = df.reset_index()\n",
    "#     df = df.set_index(\"key_index\")\n",
    "\n",
    "\n",
    "#     seg_list = []\n",
    "#     seg_courant_maxes = []\n",
    "#     for seg in all_results_new:\n",
    "#         seg_list.append(seg)\n",
    "#         seg_courant_maxes.append(max(all_results_new[seg][:, tr.courant_index]))\n",
    "#     zipped = zip(seg_list, seg_courant_maxes)\n",
    "#     zipped = list(zipped)\n",
    "#     res = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
    "#     # A.sort(reverse=True)\n",
    "#     res = (res)[:3]\n",
    "#     # print((res)[:25])\n",
    "\n",
    "#     major_segments = []\n",
    "#     for x, y in res[:3]:\n",
    "#         major_segments.append(x)\n",
    "#     # major_segments\n",
    "\n",
    "#     df_filtered = df.loc[df.index.isin(major_segments), :]\n",
    "\n",
    "#     if i == 0:\n",
    "#         df_indexed_300 = df_filtered.reset_index()\n",
    "#     elif i == 1:\n",
    "#         df_indexed_60 = df_filtered.reset_index()\n",
    "# #     else:\n",
    "# #         df_indexed_10 = df_filtered.reset_index()\n",
    "\n",
    "# test_chart = go.FigureWidget()\n",
    "# for x in range(0,3):\n",
    "#     temp_df_range_1 = single_seg_length_300 * (x)\n",
    "#     temp_df_range_2 = single_seg_length_300 * (x + 1)\n",
    "#     #         print(temp_df_range_1, temp_df_range_2)\n",
    "#     test_chart.add_scatter(\n",
    "#         x=df_indexed_300[temp_df_range_1:temp_df_range_2][\"time\"],\n",
    "#         y=df_indexed_300[temp_df_range_1:temp_df_range_2][\"flowval\"],\n",
    "#         name=\"segment \" + str(df_indexed_300[\"key_index\"][temp_df_range_1]) + \" \" + str(300),\n",
    "#     )\n",
    "#     temp_df_range_1 = single_seg_length_60 * (x)\n",
    "#     temp_df_range_2 = single_seg_length_60 * (x + 1)\n",
    "#     #         print(temp_df_range_1, temp_df_range_2)\n",
    "#     test_chart.add_scatter(\n",
    "#         x=df_indexed_60[temp_df_range_1:temp_df_range_2][\"time\"],\n",
    "#         y=df_indexed_60[temp_df_range_1:temp_df_range_2][\"flowval\"],\n",
    "#         name=\"segment \" + str(df_indexed_60[\"key_index\"][temp_df_range_1]) + \" \" + str(60),\n",
    "#     )\n",
    "#     #     temp_df_range_1 = single_seg_length_10 * (x)\n",
    "#     #     temp_df_range_2 = single_seg_length_10 * (x + 1)\n",
    "#     # #         print(temp_df_range_1, temp_df_range_2)\n",
    "#     #     test_chart.add_scatter(\n",
    "#     #         x=df_indexed_10[temp_df_range_1:temp_df_range_2][\"time\"],\n",
    "#     #         y=df_indexed_10[temp_df_range_1:temp_df_range_2][\"flowval\"],\n",
    "#     #         name=\"segment \" + str(df_indexed_10[\"key_index\"][temp_df_range_1]) + \" \" + str(dt),\n",
    "#     #     )\n",
    "\n",
    "# test_chart.layout.title = \"Timestep Chart 300, 60\"\n",
    "# plot(test_chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
